{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ZZsJHpbBRA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import itertools\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "from random import Random\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Resize, ToTensor\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the MNIST dataset"
      ],
      "metadata": {
        "id": "yCmLS06_d3la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing the image\n"
      ],
      "metadata": {
        "id": "xs2qV9753jcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "bzt_iKp93XMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Om0XiKluhw60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "Actual_train_dataset = torchvision.datasets.MNIST(root='./data',train=True,download = True,transform = transform)\n",
        "Actual_test_dataset = torchvision.datasets.MNIST(root='./data',train = False,download = True,transform = transform)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MaawgMS-dwdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Actual_train_dataset))\n",
        "print(len(Actual_test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m43AfvcArDUB",
        "outputId": "3dcab7e4-517c-4d59-8ad0-accbcac63fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classes you want to sample"
      ],
      "metadata": {
        "id": "JctCkAZFe5iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes_to_sample = [0,1,2]"
      ],
      "metadata": {
        "id": "szQLxloterM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample 100 images from each class in the training set for the VIT"
      ],
      "metadata": {
        "id": "8hvjOSpmfO8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = []\n",
        "previous_train_indices = []\n",
        "for cls in classes_to_sample:\n",
        "  class_indices = [i for i,label in enumerate(Actual_train_dataset.targets) if label == cls]\n",
        "  sampled_indices = np.random.choice(class_indices,100,replace = False)\n",
        "  previous_train_indices.append(sampled_indices)\n",
        "  # print(sampled_indices)\n",
        "  sampled_images = torch.stack([Actual_train_dataset[i][0] for i in sampled_indices])\n",
        "  train_samples.append(sampled_images)\n",
        "\n",
        "train_samples = torch.cat(train_samples,dim=0)\n",
        "# print(previous_train_indices)"
      ],
      "metadata": {
        "id": "rKwJEnFCfNBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample 100 images from each class in the test set for the VIT\n"
      ],
      "metadata": {
        "id": "cLLQQhbckc1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples=[]\n",
        "previous_test_indices=[]\n",
        "for cls in classes_to_sample:\n",
        "  class_indices = [i for i,label in enumerate(Actual_test_dataset.targets) if label == cls]\n",
        "  sampled_indices = np.random.choice(class_indices,100, replace = False)\n",
        "  previous_test_indices.append(sampled_indices)\n",
        "  sampled_images = torch.stack([Actual_test_dataset[i][0] for i in sampled_indices])\n",
        "  test_samples.append(sampled_images)\n",
        "\n",
        "# print(test_samples[0][0])\n",
        "test_samples = torch.cat(test_samples,dim=0)\n",
        "# print(len(test_samples))\n",
        "# print(previous_test_indices)"
      ],
      "metadata": {
        "id": "2Wqgkt8JgpFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the datasets to pytorch TensorDatasets\n"
      ],
      "metadata": {
        "id": "8h8optPVuWBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = torch.cat([torch.full((100,),i) for i in [0,1,2]])\n",
        "test_labels = torch.cat([torch.full((100,),i) for i in [0,1,2]])\n",
        "\n",
        "VIT_train_dataset = TensorDataset(train_samples,train_labels)\n",
        "VIT_test_dataset = TensorDataset(test_samples,test_labels)"
      ],
      "metadata": {
        "id": "yKuBYOlRgzWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPKf9SkBnZAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO see the images\n"
      ],
      "metadata": {
        "id": "LciZfWZpzvpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(VIT_train_dataset))\n",
        "print(len(VIT_test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuNc5BHbz24N",
        "outputId": "4995bb3c-a377-48fb-d59a-d920450db74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def show_mnist_images(dataset,num_images=10):\n",
        "#   plt.figure(figsize=(10,2))\n",
        "#   for i in range(num_images):\n",
        "#     image,label = dataset[i]\n",
        "#     image = image.squeeze()\n",
        "#     plt.subplot(1,num_images,i+1)\n",
        "#     plt.imshow(image,cmap='gray')\n",
        "#     plt.title(f'Label: {label}')\n",
        "#     plt.axis('off')\n",
        "#   plt.show()\n",
        "\n",
        "# show_mnist_images(VIT_train_dataset,num_images=10)"
      ],
      "metadata": {
        "id": "LxxRdeZTs9Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader\n"
      ],
      "metadata": {
        "id": "KMsrEdnGnp9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(VIT_train_dataset,batch_size=12,shuffle=True)\n",
        "test_loader = DataLoader(VIT_test_dataset,batch_size=12,shuffle=False)"
      ],
      "metadata": {
        "id": "pdOqcuTynmK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_patches(img,patch_size):\n",
        "  _,ax = plt.subplots(1,1,figsize=(2,2))\n",
        "  img = img.squeeze(0)\n",
        "  ax.imshow(img,cmap='gray')\n",
        "  ax.set_xticks(range(0,img.shape[0],patch_size))\n",
        "  ax.set_yticks(range(0,img.shape[1],patch_size))\n",
        "  ax.grid(color='white',linestyle='-',linewidth=2)\n",
        "  plt.show()\n",
        "\n",
        "n_to_viuslize =0\n",
        "patch_size=7\n",
        "for image,_ in train_loader:\n",
        "  visualize_patches(image[0],patch_size)\n",
        "  n_to_viuslize+=1\n",
        "  if(n_to_viuslize==2):\n",
        "    break"
      ],
      "metadata": {
        "id": "PfjLV-ZunVNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "5328fa1a-16fd-4eae-9199-9c5e9446ec05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pn03u9esneDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters\n"
      ],
      "metadata": {
        "id": "uhHZBEixpMyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 28\n",
        "patch_size = 7\n",
        "num_classes = 3\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "1JiV9Ywqg3Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vision Transformer\n"
      ],
      "metadata": {
        "id": "qqHrM_Wgxyoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding_conv(nn.Module):\n",
        "  def __init__(self,img_size,patch_size,in_channels=1,embed_dim=128):\n",
        "    super(PatchEmbedding_conv,self).__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.proj = nn.Conv2d(in_channels,embed_dim,kernel_size=patch_size,stride=patch_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.proj(x)\n",
        "    x=rearrange(x,'b e h w -> b (h w) e')\n",
        "    return x\n",
        "\n",
        "patch_embedding = PatchEmbedding_conv(img_size,patch_size)\n",
        "sample_image = next(iter(train_loader))[0][0].unsqueeze(0)\n",
        "patches = patch_embedding(sample_image)\n",
        "print(patches.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeIT2fPSnw4T",
        "outputId": "b270dc97-f546-4df3-8cee-736ea732552c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        B, N, C = x.shape\n",
        "        q = self.query(x).view(B, N, self.num_heads, self.head_dim)\n",
        "        k = self.key(x).view(B, N, self.num_heads, self.head_dim)\n",
        "        v = self.value(x).view(B, N, self.num_heads, self.head_dim)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n h d -> b h n d'), (q, k, v))\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        # print(attn.shape)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.out(out)\n",
        "        return out,attn\n",
        "\n",
        "# Test Multi-Head Self-Attention\n",
        "mhsa = MultiHeadSelfAttention(embed_dim=128, num_heads=8)\n",
        "attn_output,attn = mhsa(patches)\n",
        "# print(attn.shape)\n",
        "print(attn_output.shape)  # Expected shape: (1, num_patches, embed_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjFIr0bhpgJW",
        "outputId": "15739407-78a7-4ff3-ed56-7894bc9c0d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out,attn=self.mhsa(self.norm1(x))\n",
        "        x = x + self.dropout1(attn_out)\n",
        "        x = x + self.dropout2(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "# Test Transformer Encoder Block\n",
        "encoder_block = TransformerEncoderBlock(embed_dim=128, num_heads=8, mlp_dim=256)\n",
        "encoded_output = encoder_block(attn_output)\n",
        "print(encoded_output.shape)  # Expected shape: (1, num_patches, embed_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Zngb_7qGeZ",
        "outputId": "7db44a05-5d3e-422f-ad81-7509bbb55f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=28, patch_size=7, num_classes=3, embed_dim=128, depth=2, num_heads=8, mlp_dim=256,dropout=0):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_embedding = PatchEmbedding_conv(img_size, patch_size, embed_dim=embed_dim)\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim,dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x,con=False):\n",
        "        x = self.patch_embedding(x)\n",
        "        batch_size = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        # print(x.shape)\n",
        "        x += self.pos_embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for encoder_block in self.encoder:\n",
        "            x = encoder_block(x)\n",
        "\n",
        "        if con:\n",
        "          return x[:,0]\n",
        "        x = self.mlp_head(x[:, 0])\n",
        "        return x\n",
        "\n",
        "# Test Vision Transformer\n",
        "vit = VisionTransformer()\n",
        "output = vit(sample_image)\n",
        "print(output.shape)  # Expected shape: (batch_size, num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMn-BJduqJoD",
        "outputId": "0ec53333-c527-447d-8523-d8bf63bc3171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import M\n",
        "def train_model(model, train_loader, criterion, optimizer,schedular,num_epochs,num_batch_size):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            accuracy = labels.eq(F.softmax(outputs,dim=1).argmax(1)).sum().div(num_batch_size).mul(100)\n",
        "            running_loss += loss.item()\n",
        "            running_accuracy += accuracy.item()\n",
        "        schedular.step()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs},Accuracy: {running_accuracy/len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "WFfvfEwL0nKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(labels)\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).float().mean().item()\n",
        "    print(f\"Classification Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    cm = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2], yticklabels=[0, 1, 2])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(all_labels.numpy(), all_preds.numpy()))\n"
      ],
      "metadata": {
        "id": "QoGW9J3m1GQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hperparameters"
      ],
      "metadata": {
        "id": "K5UhH5Jo53iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "num_epochs = 10\n",
        "num_batches = 12\n",
        "hidden_dim = 512\n",
        "# embed_dim ="
      ],
      "metadata": {
        "id": "i2IwjKDX1Iwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "schedular = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)"
      ],
      "metadata": {
        "id": "fbP1mj6X1Ldr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train VIT"
      ],
      "metadata": {
        "id": "sAqMZlgkZdi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, criterion, optimizer,schedular, num_epochs,num_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeJVYJjY1NlC",
        "outputId": "f71d1f9f-9f62-4c55-e91f-8ec4b09716ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.1499717807769776\n",
            "Epoch 1/10,Accuracy: 31.00000030517578\n",
            "Epoch 2/10, Loss: 1.0811369013786316\n",
            "Epoch 2/10,Accuracy: 40.0\n",
            "Epoch 3/10, Loss: 1.0549595141410828\n",
            "Epoch 3/10,Accuracy: 48.666666717529296\n",
            "Epoch 4/10, Loss: 0.9593509793281555\n",
            "Epoch 4/10,Accuracy: 63.00000015258789\n",
            "Epoch 5/10, Loss: 0.6777983808517456\n",
            "Epoch 5/10,Accuracy: 81.0\n",
            "Epoch 6/10, Loss: 0.4007666563987732\n",
            "Epoch 6/10,Accuracy: 87.66666564941406\n",
            "Epoch 7/10, Loss: 0.262288773059845\n",
            "Epoch 7/10,Accuracy: 89.33333435058594\n",
            "Epoch 8/10, Loss: 0.20244837746024133\n",
            "Epoch 8/10,Accuracy: 92.00000061035156\n",
            "Epoch 9/10, Loss: 0.14570129454135894\n",
            "Epoch 9/10,Accuracy: 95.66666748046875\n",
            "Epoch 10/10, Loss: 0.12458824388682842\n",
            "Epoch 10/10,Accuracy: 95.00000183105469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"vit_model.pth\")"
      ],
      "metadata": {
        "id": "odwHtkkVbS7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing model parameters\n"
      ],
      "metadata": {
        "id": "YWySDstqZiAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for p in model.parameters():\n",
        "#   if p.grad is not None:\n",
        "#     print(p.grad.data.abs().mean())"
      ],
      "metadata": {
        "id": "Dwq6O2I4fWWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation VIT and making confusion Matrix"
      ],
      "metadata": {
        "id": "tRQ6tjsdZm1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "8RxskVW21mkJ",
        "outputId": "e51f33e1-6a40-4dac-dddb-7876a007182a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy: 94.33%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96       100\n",
            "           1       0.95      0.96      0.96       100\n",
            "           2       0.97      0.87      0.92       100\n",
            "\n",
            "    accuracy                           0.94       300\n",
            "   macro avg       0.94      0.94      0.94       300\n",
            "weighted avg       0.94      0.94      0.94       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IrqPjVxw8Y2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT with Regularization and Data Augmentation\n",
        "\n",
        "1. Regularization\n",
        "\n",
        "\n",
        "*   Weight decay\n",
        "*   Dropout\n",
        "\n",
        "2. Data Augmentation\n",
        "\n",
        "\n",
        "*   MixUp\n",
        "*   RandAugment\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xJj6NZi66w8V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRAr_tXU65dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"num_epochs\": 10,\n",
        "    \"hidden_dim\": 256,\n",
        "    \"num_heads\": 4,\n",
        "    \"hidden_embedding_size\": 256,\n",
        "    \"dropout\": 0.2,\n",
        "    \"stochastic_depth_probability\": 0.2,\n",
        "}"
      ],
      "metadata": {
        "id": "z0cPBQXG-X6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=0.0001)\n",
        "schedular = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)"
      ],
      "metadata": {
        "id": "f2Rc511S-kAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha=1.0):\n",
        "  if alpha > 0:\n",
        "    lam = torch.distributions.Beta(alpha, alpha).sample()\n",
        "  else:\n",
        "    lam = 1.0\n",
        "  batch_size = x.size(0)\n",
        "  index = torch.randperm(batch_size)\n",
        "\n",
        "  mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "  mixed_y = lam * y + (1 - lam) * y[index]\n",
        "  return mixed_x[0], mixed_y.long()[0]"
      ],
      "metadata": {
        "id": "YQwIW4IG7kpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rand_augment(image, num_ops=2, magnitude=9):\n",
        "  augmentations = [\n",
        "    torchvision.transforms.RandomRotation(degrees=(0, magnitude * 10)),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "    torchvision.transforms.RandomResizedCrop(size=image.shape[-2:], scale=(0.8, 1.0)),\n",
        "    torchvision.transforms.ColorJitter(brightness=magnitude/10, contrast=magnitude/10),\n",
        "    torchvision.transforms.RandomAffine(degrees=(0, 0), translate=(magnitude/20, magnitude/20)),\n",
        "  ]\n",
        "\n",
        "  transform_list = torch.randperm(len(augmentations))[:num_ops].tolist()\n",
        "\n",
        "  for i in transform_list:\n",
        "    image = augmentations[i](image)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "hvoOC4nj8jlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_aug(Network,\n",
        "              number_of_epoch,\n",
        "              dataloader,\n",
        "              criterion,\n",
        "              optimizer,\n",
        "              hyperparameters,\n",
        "              ):\n",
        "    Network.train()\n",
        "\n",
        "    # _save_hyperparameters_to_pdf(hyperparameters,pdf)\n",
        "\n",
        "    for epoch in range(number_of_epoch):\n",
        "      running_loss = 0.0\n",
        "      all_preds = []\n",
        "      all_labels = []\n",
        "      for data, label in dataloader:\n",
        "        if torch.rand(1).item() > 0.5:\n",
        "          data, label = mixup_data(data.unsqueeze(0), label.unsqueeze(0))\n",
        "        else:\n",
        "          data = rand_augment(data)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Network(data)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(label)\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "      all_preds = torch.cat(all_preds)\n",
        "      all_labels = torch.cat(all_labels)\n",
        "      accuracy = (all_preds == all_labels).float().mean().item()\n",
        "\n",
        "      clear_output(wait=True)\n",
        "      print(f\"Epoch {epoch+1} Loss: {running_loss/len(dataloader)} Accuracy: {accuracy* 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "WpTioQAL8qO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_aug(Network=model,\n",
        "          number_of_epoch=hyperparameters['num_epochs'],\n",
        "          dataloader=train_loader,\n",
        "          criterion = criterion,\n",
        "          optimizer = optimizer,\n",
        "          hyperparameters=hyperparameters\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOy2hfmX8_Qn",
        "outputId": "c4e8a558-b781-429b-e872-c0c37ea8ede4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Loss: 0.4763642412424087 Accuracy: 81.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate( Network,\n",
        "              dataloader,\n",
        "              ):\n",
        "  Network.eval()\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, label in dataloader:\n",
        "      outputs = Network(data)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      all_preds.append(preds)\n",
        "      all_labels.append(label)\n",
        "  all_preds = torch.cat(all_preds)\n",
        "  all_labels = torch.cat(all_labels)\n",
        "\n",
        "  accuracy = (all_preds == all_labels).float().mean().item()\n",
        "  print(f\"Classification Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "  matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
        "  fig, ax = plt.subplots()\n",
        "  sns.heatmap(matrix, annot=True, fmt='d', cmap='jet', xticklabels=[0, 1, 2], yticklabels=[0, 1, 2], ax=ax)\n",
        "  ax.set_xlabel('Predicted')\n",
        "  ax.set_ylabel('True')\n",
        "  ax.set_title(f'Confusion Matrix - Accuracy: {accuracy * 100:.2f}%')\n",
        "  # pdf.savefig(fig)\n",
        "  plt.close(fig)\n",
        "\n",
        "  classification_rep = classification_report(all_labels.numpy(), all_preds.numpy())\n",
        "  # self._save_text_to_pdf(\"Classification Report\", classification_rep)\n"
      ],
      "metadata": {
        "id": "QAH9NN75Ai5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(Network=model,\n",
        "         dataloader = test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNkJ0eTOAIVI",
        "outputId": "a09ba78e-8fd9-44d5-8dcf-1b32f829e3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy: 92.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contrastive LOSS"
      ],
      "metadata": {
        "id": "Rt5hDdMUEGKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,out_1, out_2, label):\n",
        "    distance = torch.nn.functional.pairwise_distance(out_1,out_2)\n",
        "    loss = torch.mean((1-label)*torch.pow(distance,2) + label*torch.pow(torch.clamp(1 - distance, min=0.0), 2))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "EgNGpC8bEJLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer()\n",
        "criterion = ContrastiveLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=0.0001)\n",
        "schedular = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)"
      ],
      "metadata": {
        "id": "YbBD40gEERnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "fZiMeqFeVFEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_con(Network,\n",
        "                number_of_epoch,\n",
        "                dataloader,\n",
        "                criterion,\n",
        "                optimizer):\n",
        "    Network.train()\n",
        "\n",
        "    for epoch in range(number_of_epoch):\n",
        "      running_loss = 0.0\n",
        "      for data, _ in dataloader:\n",
        "        data2 = rand_augment(data,num_ops=1)\n",
        "\n",
        "        data_pairs = list(itertools.product(data, data2))\n",
        "        data1 = torch.stack([pair[0] for pair in data_pairs])\n",
        "        data2 = torch.stack([pair[1] for pair in data_pairs])\n",
        "\n",
        "        label = torch.ones(data1.size(0))\n",
        "        batch_size = data.size(0)\n",
        "        for i in range(batch_size):\n",
        "          label[i * batch_size + i] = 0\n",
        "\n",
        "        similar_indices = (label == 0).nonzero(as_tuple=True)[0]\n",
        "        dissimilar_indices = (label == 1).nonzero(as_tuple=True)[0]\n",
        "\n",
        "        sampled_dissimilar_indices = random.sample(dissimilar_indices.tolist(), len(similar_indices))\n",
        "        indices = torch.tensor(similar_indices.tolist() + sampled_dissimilar_indices).to(data.device)\n",
        "\n",
        "        data1 = data1[indices]\n",
        "        data2 = data2[indices]\n",
        "        label = label[indices]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs1 = Network(data1,con = True)\n",
        "        outputs2 = Network(data2,con = True)\n",
        "\n",
        "        loss = criterion(outputs1, outputs2, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "      # clear_output(wait=True)\n",
        "      print(f\"Epoch {epoch+1} Loss: {running_loss/len(dataloader)}\")"
      ],
      "metadata": {
        "id": "TaVOtrIMUj5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_con(model,2,train_loader,criterion,optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDTMJTQ9VH3z",
        "outputId": "f149572c-9a41-4293-9832-f2b387c2658e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 21.017184219360352\n",
            "Epoch 2 Loss: 19.72689147949219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SWIM DATASET"
      ],
      "metadata": {
        "id": "5nqfQVQ_8Z9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSVlVIJr6B9t",
        "outputId": "858b3f57-d7b0-441c-ead8-80e3fc8e83c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m30.7/42.4 kB\u001b[0m \u001b[31m652.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m653.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
      ],
      "metadata": {
        "id": "e3mz7BI2k6t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_train_indices = np.array(previous_train_indices)\n",
        "previous_test_indices = np.array(previous_test_indices)\n",
        "# print(previous_train_indices)"
      ],
      "metadata": {
        "id": "6HoNn4aD3W9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_indices = {0:[],1:[],2:[]}\n",
        "new_test_indices = {0:[],1:[],2:[]}"
      ],
      "metadata": {
        "id": "gULs_qPv4zcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,(image,label) in enumerate(Actual_train_dataset):\n",
        "  if label in [0,1,2] and i not in previous_train_indices:\n",
        "    new_train_indices[label].append(i)"
      ],
      "metadata": {
        "id": "7eHVnO_p3ClA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,(image,label) in enumerate(Actual_test_dataset):\n",
        "  if label in [0,1,2] and i not in previous_train_indices:\n",
        "    new_test_indices[label].append(i)"
      ],
      "metadata": {
        "id": "8Zu8EN9F5_dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_train_indices = []\n",
        "selected_test_indices = []\n",
        "\n",
        "for cls in classes_to_sample:\n",
        "  selected_train_indices.extend(np.random.choice(new_train_indices[cls],100,replace=False))\n",
        "  selected_test_indices.extend(np.random.choice(new_test_indices[cls],100,replace=False))"
      ],
      "metadata": {
        "id": "URhjZsyf6IRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Swim_train_samples = torch.utils.data.Subset(Actual_train_dataset,selected_train_indices)\n",
        "Swim_test_samples = torch.utils.data.Subset(Actual_test_dataset,selected_test_indices)"
      ],
      "metadata": {
        "id": "xIZTLoLx677V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Swim_train_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UoNap_P_DiK",
        "outputId": "0401b74a-27ef-4812-8fdd-19affa2f17f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Swim_train_loader = torch.utils.data.DataLoader(Swim_train_samples,batch_size=12,shuffle=True)\n",
        "Swim_test_loader = torch.utils.data.DataLoader(Swim_test_samples,batch_size=12,shuffle=True)"
      ],
      "metadata": {
        "id": "YOstdMyK7fy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, window_size):\n",
        "\n",
        "    B, H, W, C = x.shape\n",
        "    pad_h = (window_size - H % window_size) % window_size\n",
        "    pad_w = (window_size - W % window_size) % window_size\n",
        "\n",
        "    if pad_h > 0 or pad_w > 0:\n",
        "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
        "\n",
        "    H_padded, W_padded = x.shape[1], x.shape[2]\n",
        "\n",
        "    x = x.view(B, H_padded // window_size, window_size, W_padded // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "\n",
        "    return windows\n"
      ],
      "metadata": {
        "id": "u4_-Wof5Fq6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=28, patch_size=7, in_chans=1, embed_dim=128, norm_layer=None):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_resolution = patches_resolution\n",
        "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}]) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_lM-mBtfF1Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
        "    x = x.view(B, H, W, -1)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "WWSj2CnkGF1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act_layer = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_layer(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "bDnmtcQwGMNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
        "        )\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n",
        "        coords_flatten = torch.flatten(coords, 1)\n",
        "\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "\n",
        "        self.register_buffer('relative_position_index', relative_position_index)\n",
        "\n",
        "        # Attention\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B_, N, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "\n",
        "        qkv = qkv.reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
        "\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1],  -1\n",
        "        )\n",
        "\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Ll8S0SB7GeV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = self.input_resolution\n",
        "        B, L,C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
        "\n",
        "        x = x.view(B, H, W, C)\n",
        "        x0 = x[:, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, :]\n",
        "        x3 = x[:, 1::2, 1::2, :]\n",
        "\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)\n",
        "\n",
        "        x = x.view(B, -1, 4 * C)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "euqhJkYmHG6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = min(window_size, min(input_resolution))  # Ensure window_size is valid\n",
        "        self.shift_size = shift_size if shift_size < self.window_size else 0  # Ensure shift_size is within bounds\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in 0-window_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(\n",
        "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop\n",
        "        )\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        # Calculate attention mask for SW-MSA\n",
        "        if self.shift_size > 0:\n",
        "            H, W = self.input_resolution\n",
        "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
        "            h_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            cnt = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    img_mask[:, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "\n",
        "            mask_windows = window_partition(img_mask, self.window_size)\n",
        "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        self.register_buffer(\"attn_mask\", attn_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # Partition windows\n",
        "        x_windows = window_partition(shifted_x, self.window_size)\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
        "\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
        "\n",
        "        # Merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
        "\n",
        "        # Reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bwQQWVs0OTuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleSwinTransformer(nn.Module):\n",
        "    def __init__(self, img_size=28, patch_size=4, in_chans=1, num_classes=3,\n",
        "                 embed_dim=96, depths=[2, 2], num_heads=[3, 6], window_size=7,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ape = ape\n",
        "        self.patch_norm = patch_norm\n",
        "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # Split image into non-overlapping patches\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if self.patch_norm else None\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        patches_resolution = self.patch_embed.patches_resolution\n",
        "        self.patches_resolution = patches_resolution\n",
        "\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # Stochastic Depth\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] # stochastic depth decay rule\n",
        "\n",
        "        # Build Swin Transformer layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = BasicLayer(\n",
        "                dim=int(embed_dim * 2 ** i_layer),\n",
        "                input_resolution=(\n",
        "                    patches_resolution[0] // (2 ** i_layer), # After patch-merging layer, patches_resolution(H, W) is halved\n",
        "                    patches_resolution[1] // (2 ** i_layer),\n",
        "                ),\n",
        "                depth=depths[i_layer],\n",
        "                num_heads=num_heads[i_layer],\n",
        "                window_size=window_size,\n",
        "                mlp_ratio=self.mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None, # No patch merging at the last stage\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classification Head\n",
        "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.norm(x) # (B, L, C)\n",
        "        x = self.avgpool(x.transpose(1, 2)) # (B, C, 1)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3dmMHoKxOYSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BasicLayer(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        # Build  Swin-Transformer Blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
        "                                 num_heads=num_heads, window_size=window_size,\n",
        "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                                 mlp_ratio = mlp_ratio,\n",
        "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                 drop=drop, attn_drop=attn_drop,\n",
        "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                 norm_layer=norm_layer\n",
        "                                )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "\n",
        "        # Patch Merging Layer\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1v_Q9n6bHwDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stage 1\n"
      ],
      "metadata": {
        "id": "dkGN4A9LioI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stage_1 = BasicLayer(dim=128, input_resolution=(4, 4), depth=2, num_heads=4, window_size=7)\n",
        "\n",
        "# Pass the patch embeddings through the BasicLayer\n",
        "output = stage_1(patches)\n",
        "print(output.shape)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLTO-DANinBU",
        "outputId": "5da706a7-a57b-4b36-dfdd-c10f6f01875c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 128])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4190, -0.7563, -0.8105,  ...,  0.6297,  0.4798, -0.5649],\n",
              "         [-0.3977, -0.8002, -0.7496,  ...,  0.6237,  0.4607, -0.5449],\n",
              "         [-1.3915, -0.9089, -0.7139,  ...,  0.1024,  0.6572, -0.4055],\n",
              "         ...,\n",
              "         [ 0.5752,  0.2125, -0.3761,  ...,  0.5891, -0.0957, -0.3118],\n",
              "         [ 0.0665, -0.3091, -0.7166,  ...,  1.0115,  0.1487, -0.6597],\n",
              "         [-0.3602, -0.8059, -0.9385,  ...,  0.8065,  0.5675, -0.5291]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge Distillation"
      ],
      "metadata": {
        "id": "M0bwV24yaThi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the KL Divergence loss with temperature scaling\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=4):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, student_outputs, teacher_outputs):\n",
        "        soft_teacher_outputs = F.softmax(teacher_outputs / self.temperature, dim=1)\n",
        "        soft_student_outputs = F.log_softmax(student_outputs / self.temperature, dim=1)\n",
        "        loss = F.kl_div(soft_student_outputs, soft_teacher_outputs, reduction='batchmean')\n",
        "        return loss\n",
        "\n",
        "# Instantiate the distillation loss\n",
        "distillation_loss = DistillationLoss(temperature=4)"
      ],
      "metadata": {
        "id": "bqf2_UsWaTN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Swim"
      ],
      "metadata": {
        "id": "fAOVU22MaCaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example instantiation\n",
        "swin_model = SimpleSwinTransformer(img_size=28, patch_size=4, in_chans=1, num_classes=3,\n",
        "                                   embed_dim=96, depths=[2, 2], num_heads=[3, 6], window_size=7)\n"
      ],
      "metadata": {
        "id": "S2_YnWguIYo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the Swin Transformer model (make sure this matches the final model you implemented)\n",
        "swin_model = SimpleSwinTransformer(\n",
        "    img_size=28,\n",
        "    patch_size=2,\n",
        "    in_chans=1,\n",
        "    num_classes=3,\n",
        "    embed_dim=96,\n",
        "    depths=[2, 2],\n",
        "    num_heads=[3, 6],\n",
        "    window_size=7\n",
        ")\n",
        "\n",
        "# Create a dummy input tensor\n",
        "# Simulate a batch of 2 images, each with 1 channel, and size 28x28\n",
        "dummy_input = torch.randn(2, 1, 28, 28)\n",
        "\n",
        "# Pass the dummy input through the model\n",
        "output = swin_model(dummy_input)\n",
        "\n",
        "# Print out the shape of the output tensor\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWKox-fSNqVq",
        "outputId": "75d9f069-18e4-4335-9b64-51356f5d41c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assume vit_model is your pre-trained Vision Transformer model\n",
        "vit_model = VisionTransformer()  # example configuration\n",
        "vit_model.load_state_dict(torch.load(\"/content/vit_model.pth\"))  # Load pre-trained ViT weights\n",
        "vit_model.eval()\n",
        "\n",
        "# Training settings\n",
        "num_epochs = 30\n",
        "learning_rate = 0.0001  # Example learning rate\n",
        "optimizer = optim.Adam(swin_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    swin_model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, _ in Swim_train_loader:  # Ground truth labels are not used\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get teacher model predictions\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = vit_model(images)\n",
        "\n",
        "        # Get student model predictions\n",
        "        student_outputs = swin_model(images)\n",
        "\n",
        "        # Compute distillation loss\n",
        "        loss = distillation_loss(student_outputs, teacher_outputs)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "pMu84u7IYguQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60987bd3-a864-49fa-a3c1-36c96e397093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-249-a17b3ebc4fc1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vit_model.load_state_dict(torch.load(\"/content/vit_model.pth\"))  # Load pre-trained ViT weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.05375007566064596\n",
            "Epoch 2/30, Loss: 0.048219986353069545\n",
            "Epoch 3/30, Loss: 0.03748925087973475\n",
            "Epoch 4/30, Loss: 0.034500099327415226\n",
            "Epoch 5/30, Loss: 0.0327657494880259\n",
            "Epoch 6/30, Loss: 0.053005581311881544\n",
            "Epoch 7/30, Loss: 0.04563612747937441\n",
            "Epoch 8/30, Loss: 0.048875913638621565\n",
            "Epoch 9/30, Loss: 0.040633688289672135\n",
            "Epoch 10/30, Loss: 0.03297023698687553\n",
            "Epoch 11/30, Loss: 0.03956006467342377\n",
            "Epoch 12/30, Loss: 0.03692299088463187\n",
            "Epoch 13/30, Loss: 0.02984450859948993\n",
            "Epoch 14/30, Loss: 0.03063065208494663\n",
            "Epoch 15/30, Loss: 0.02712597830221057\n",
            "Epoch 16/30, Loss: 0.03464750539511442\n",
            "Epoch 17/30, Loss: 0.03307199776172638\n",
            "Epoch 18/30, Loss: 0.03254819005727768\n",
            "Epoch 19/30, Loss: 0.02659903798252344\n",
            "Epoch 20/30, Loss: 0.033464576583355665\n",
            "Epoch 21/30, Loss: 0.029151382744312285\n",
            "Epoch 22/30, Loss: 0.04114882891997695\n",
            "Epoch 23/30, Loss: 0.03273942470550537\n",
            "Epoch 24/30, Loss: 0.03030054096132517\n",
            "Epoch 25/30, Loss: 0.023956729490309955\n",
            "Epoch 26/30, Loss: 0.02582424646243453\n",
            "Epoch 27/30, Loss: 0.025766992764547466\n",
            "Epoch 28/30, Loss: 0.031991244815289976\n",
            "Epoch 29/30, Loss: 0.02733114695176482\n",
            "Epoch 30/30, Loss: 0.02395142389461398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation SWIM"
      ],
      "metadata": {
        "id": "LM78LOFhbA7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in Swim_test_loader:\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(labels)\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).float().mean().item()\n",
        "    print(f\"Classification Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    cm = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2], yticklabels=[0, 1, 2])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(all_labels.numpy(), all_preds.numpy()))\n",
        "\n",
        "# Evaluate Swin model on the test set\n",
        "evaluate_model(swin_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "i0OX1HqBaEi2",
        "outputId": "51b8d2c4-eb39-4508-b950-e04999b6d6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy: 92.67%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.91      0.93       100\n",
            "           1       0.93      0.99      0.96       100\n",
            "           2       0.90      0.88      0.89       100\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.93      0.93      0.93       300\n",
            "weighted avg       0.93      0.93      0.93       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P80DOaK3jUPE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}